{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time, os, sys, re, gc, json, multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import emoji, random, unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "```python\n",
    "{\n",
    "    \"org123\": {\n",
    "        \"agriculture\": 1,\n",
    "        \"cross\": 2,\n",
    "        \"education\": 3,\n",
    "        \"food\": 4,\n",
    "        \"health\": 5,\n",
    "        \"livelihood\": 6,\n",
    "        \"logistic\": 7,\n",
    "        \"nfi\": 8,\n",
    "        \"nutrition\": 9,\n",
    "        \"protection\": 10,\n",
    "        \"shelter\": 11,\n",
    "        \"wash\": 12\n",
    "    },\n",
    "    \"org4\": {\n",
    "        \"Child Protection\": 101,\n",
    "        \"Early Recovery and Livelihood\": 102,\n",
    "        \"Education\": 103,\n",
    "        \"Food\": 104,\n",
    "        \"GBV\": 105,\n",
    "        \"Health\": 106,\n",
    "        \"Logistic\": 107,\n",
    "        \"Mine Action\": 108,\n",
    "        \"Nutrition\": 109,\n",
    "        \"Protection\": 110,\n",
    "        \"Shelter and NFIs\": 111,\n",
    "        \"WASH\": 112\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org1 = pd.read_csv('data/org1_dev.csv')\n",
    "org2 = pd.read_csv('data/org2_dev.csv')\n",
    "org3 = pd.read_csv('data/org3_dev.csv')\n",
    "test1 = pd.read_csv('data/org1_test.csv')\n",
    "test2 = pd.read_csv('data/org2_test.csv')\n",
    "test3 = pd.read_csv('data/org3_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of ORG1 : \", org1.shape)\n",
    "print(\"Length of ORG2 : \", org2.shape)\n",
    "print(\"Length of ORG3 : \", org3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org1.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org1.loc[org1.language == 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org1.labels.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_try = org1.copy()\n",
    "org_try = org_try[:5]\n",
    "org_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,13):\n",
    "    org_try[str(i)] = org_try['labels'].apply(lambda x: 1 if str(i) in x else 0)\n",
    "org_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,13): \n",
    "    org1[str(i)] = org1['labels'].apply(lambda x: 1 if str(i) in x else 0)\n",
    "    org2[str(i)] = org1['labels'].apply(lambda x: 1 if str(i) in x else 0)\n",
    "    org3[str(i)] = org1['labels'].apply(lambda x: 1 if str(i) in x else 0)\n",
    "    test1[str(i)] = 0\n",
    "    test2[str(i)] = 0\n",
    "    test3[str(i)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have one hot encoded values for the labels. We have to now ignore column labels and make predictions on the 12 created columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = org1.drop(['id', 'entry_original','labels','language','entry_translated'], axis=1)\n",
    "counts = []\n",
    "categories = list(df.columns.values)\n",
    "for i in categories:\n",
    "    counts.append((i, df[i].sum()))\n",
    "df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [str(i) for i in range(1,13)]\n",
    "label_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "symbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
    "CONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "CUSTOM_TABLE = str.maketrans(\n",
    "    {\n",
    "        \"\\xad\": None,\n",
    "        \"\\x7f\": None,\n",
    "        \"\\ufeff\": None,\n",
    "        \"\\u200b\": None,\n",
    "        \"\\u200e\": None,\n",
    "        \"\\u202a\": None,\n",
    "        \"\\u202c\": None,\n",
    "        \"â€˜\": \"'\",\n",
    "        \"â€™\": \"'\",\n",
    "        \"`\": \"'\",\n",
    "        \"â€œ\": '\"',\n",
    "        \"â€\": '\"',\n",
    "        \"Â«\": '\"',\n",
    "        \"Â»\": '\"',\n",
    "        \"É¢\": \"G\",\n",
    "        \"Éª\": \"I\",\n",
    "        \"É´\": \"N\",\n",
    "        \"Ê€\": \"R\",\n",
    "        \"Ê\": \"Y\",\n",
    "        \"Ê™\": \"B\",\n",
    "        \"Êœ\": \"H\",\n",
    "        \"ÊŸ\": \"L\",\n",
    "        \"Ò“\": \"F\",\n",
    "        \"á´€\": \"A\",\n",
    "        \"á´„\": \"C\",\n",
    "        \"á´…\": \"D\",\n",
    "        \"á´‡\": \"E\",\n",
    "        \"á´Š\": \"J\",\n",
    "        \"á´‹\": \"K\",\n",
    "        \"á´\": \"M\",\n",
    "        \"Îœ\": \"M\",\n",
    "        \"á´\": \"O\",\n",
    "        \"á´˜\": \"P\",\n",
    "        \"á´›\": \"T\",\n",
    "        \"á´œ\": \"U\",\n",
    "        \"á´¡\": \"W\",\n",
    "        \"á´ \": \"V\",\n",
    "        \"Ä¸\": \"K\",\n",
    "        \"Ð²\": \"B\",\n",
    "        \"Ð¼\": \"M\",\n",
    "        \"Ð½\": \"H\",\n",
    "        \"Ñ‚\": \"T\",\n",
    "        \"Ñ•\": \"S\",\n",
    "        \"â€”\": \"-\",\n",
    "        \"â€“\": \"-\",\n",
    "    }\n",
    ")\n",
    "\n",
    "NMS_TABLE = dict.fromkeys(\n",
    "    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n",
    ")\n",
    "\n",
    "HEBREW_TABLE = {i: \"×\" for i in range(0x0590, 0x05FF)}\n",
    "ARABIC_TABLE = {i: \"Ø§\" for i in range(0x0600, 0x06FF)}\n",
    "CHINESE_TABLE = {i: \"æ˜¯\" for i in range(0x4E00, 0x9FFF)}\n",
    "KANJI_TABLE = {i: \"ãƒƒ\" for i in range(0x2E80, 0x2FD5)}\n",
    "HIRAGANA_TABLE = {i: \"ãƒƒ\" for i in range(0x3041, 0x3096)}\n",
    "KATAKANA_TABLE = {i: \"ãƒƒ\" for i in range(0x30A0, 0x30FF)}\n",
    "\n",
    "TABLE = dict()\n",
    "TABLE.update(CUSTOM_TABLE)\n",
    "TABLE.update(NMS_TABLE)\n",
    "\n",
    "# Non-english languages\n",
    "TABLE.update(CHINESE_TABLE)\n",
    "TABLE.update(HEBREW_TABLE)\n",
    "TABLE.update(ARABIC_TABLE)\n",
    "TABLE.update(HIRAGANA_TABLE)\n",
    "TABLE.update(KATAKANA_TABLE)\n",
    "TABLE.update(KANJI_TABLE)\n",
    "\n",
    "\n",
    "RE_SPACE = re.compile(r\"\\s\")\n",
    "RE_MULTI_SPACE = re.compile(r\"\\s+\")\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "EMOJI_REGEXP = emoji.get_emoji_regexp()\n",
    "\n",
    "UNICODE_EMOJI = {\n",
    "    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n",
    "    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n",
    "}\n",
    "\n",
    "\n",
    "def remove_emoji(string: str):\n",
    "    def replace(match):\n",
    "        return UNICODE_EMOJI.get(match.group(0), match.group(0))\n",
    "\n",
    "    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str):\n",
    "    for k, v in CONTRACTION_MAPPING.items():\n",
    "        text = text.replace(' %s ' % k, ' %s ' % v)\n",
    "    text = text.translate(remove_dict)\n",
    "    text = text.translate(isolate_dict)\n",
    "    text = remove_emoji(text)\n",
    "    text = RE_SPACE.sub(\" \", text)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.translate(TABLE)\n",
    "    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n",
    "    text = re.sub(\"@[a-zA-Z]*\",\"USER\",text)\n",
    "    text = re.sub(\"#[a-zA-z]*\",\"HASHTAG\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with multiprocessing.Pool(processes=4) as pool:\n",
    "    org1_list = pool.map(normalize, org1.entry_translated.tolist())\n",
    "    org2_list = pool.map(normalize, org2.entry_translated.tolist())\n",
    "    org3_list = pool.map(normalize, org3.entry_translated.tolist())\n",
    "    test1_list = pool.map(normalize, test1.entry_translated.tolist())\n",
    "    test2_list = pool.map(normalize, test2.entry_translated.tolist())\n",
    "    test3_list = pool.map(normalize, test3.entry_translated.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org1['text'] = org1_list\n",
    "org2['text'] = org2_list\n",
    "org3['text'] = org3_list\n",
    "test1['text'] = test1_list\n",
    "test2['text'] = test2_list\n",
    "test3['text'] = test3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org123 = pd.concat([org1, org2, org3])\n",
    "test123 = pd.concat([test1, test2, test3])\n",
    "print(org123.shape)\n",
    "print(test123.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org123.to_csv('data/train123.csv',index=False)\n",
    "test123.to_csv('data/test123.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "               min_df=5, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, max_features=50000,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "org1_tfidf = word_vectorizer.fit_transform(org1['text'])\n",
    "org2_tfidf = word_vectorizer.transform(org2['text'])\n",
    "org3_tfidf = word_vectorizer.transform(org3['text'])\n",
    "test1_tfidf = word_vectorizer.transform(test1['text'])\n",
    "test2_tfidf = word_vectorizer.transform(test2['text'])\n",
    "test3_tfidf = word_vectorizer.transform(test3['text'])\n",
    "org123_tfidf = word_vectorizer.fit_transform(org123['text'])\n",
    "test123_tfidf = word_vectorizer.transform(test123['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a model one dependent at a time\n",
    "def get_model(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True, solver='liblinear')\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1 \n",
    "test_x = test1_tfidf\n",
    "x = org1_tfidf\n",
    "train = org1\n",
    "preds = np.zeros((len(test1), len(label_cols)))\n",
    "print(preds.shape)\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_model(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submid = pd.DataFrame({'id': test1[\"id\"]})\n",
    "submission1 = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "values=[]\n",
    "for i in range(len(test1)): \n",
    "    values.append(np.argmax(preds[i]) + 1)\n",
    "submission1['predicted_label'] = values\n",
    "del values\n",
    "gc.collect()\n",
    "submission1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 2\n",
    "test_x = test2_tfidf\n",
    "x = org2_tfidf\n",
    "train = org2\n",
    "preds = np.zeros((len(test2), len(label_cols)))\n",
    "print(preds.shape)\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_model(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submid = pd.DataFrame({'id': test2[\"id\"]})\n",
    "submission2 = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "values=[]\n",
    "for i in range(len(test2)): \n",
    "    values.append(np.argmax(preds[i]) + 1)\n",
    "submission2['predicted_label'] = values\n",
    "del values\n",
    "gc.collect()\n",
    "submission2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 3\n",
    "test_x = test3_tfidf\n",
    "x = org3_tfidf\n",
    "train = org3\n",
    "preds = np.zeros((len(test3), len(label_cols)))\n",
    "print(preds.shape)\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_model(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submid = pd.DataFrame({'id': test3[\"id\"]})\n",
    "submission3 = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "values=[]\n",
    "for i in range(len(test3)): \n",
    "    values.append(np.argmax(preds[i]) + 1)\n",
    "submission3['predicted_label'] = values\n",
    "del values\n",
    "gc.collect()\n",
    "submission3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission1.shape)\n",
    "print(submission2.shape)\n",
    "print(submission3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission123 = pd.concat([submission1, submission2, submission3])\n",
    "print(submission123.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission123.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission123.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 123 all combined as one test set\n",
    "test_x = test123_tfidf\n",
    "x = org123_tfidf\n",
    "train = org123\n",
    "preds = np.zeros((len(test123), len(label_cols)))\n",
    "print(preds.shape)\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_model(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submid = pd.DataFrame({'id': test123[\"id\"]})\n",
    "submid = submid.reset_index()\n",
    "# print(sorted(submid.index))\n",
    "submission123_combined = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "values=[]\n",
    "for i in range(len(test123)): \n",
    "    values.append(np.argmax(preds[i]) + 1)\n",
    "submission123_combined['predicted_label'] = values\n",
    "del values\n",
    "gc.collect()\n",
    "submission123_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORG4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4 = pd.read_csv('data/org4_dev.csv')\n",
    "test4 = pd.read_csv('data/org4_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4_cols = [str(i) for i in range(101,113)]\n",
    "org4_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(101,113): \n",
    "    org4[str(i)] = org4['labels'].apply(lambda x: 1 if str(i) in x else 0)\n",
    "    test4[str(i)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with multiprocessing.Pool(processes=4) as pool:\n",
    "    org4_list = pool.map(normalize, org4.entry_translated.tolist())\n",
    "    test4_list = pool.map(normalize, test4.entry_translated.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4['text'] = org4_list\n",
    "test4['text'] = test4_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4.to_csv('data/org4.csv',index=False)\n",
    "test4.to_csv('data/test4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4 = pd.read_csv('data/org4.csv')\n",
    "test4 = pd.read_csv('data/test4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "org4_tfidf = word_vectorizer.fit_transform(org4['text'])\n",
    "test4_tfidf = word_vectorizer.transform(test4['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org4['107'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence 107 doesn't even come in the test set once. So we can effectively ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1 \n",
    "test_x = test4_tfidf\n",
    "x = org4_tfidf\n",
    "train = org4\n",
    "preds = np.zeros((len(test4), len(org4_cols)))\n",
    "print(preds.shape)\n",
    "\n",
    "for i, j in enumerate(org4_cols):\n",
    "    print('fit', j)\n",
    "    if j == '107': \n",
    "        preds[:,i] = 0\n",
    "        continue\n",
    "    m,r = get_model(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submid = pd.DataFrame({'id': test4[\"id\"]})\n",
    "submission4 = pd.concat([submid, pd.DataFrame(preds, columns = org4_cols)], axis=1)\n",
    "values=[]\n",
    "for i in range(len(test4)): \n",
    "    values.append(np.argmax(preds[i]) + 101)\n",
    "submission4['predicted_label'] = values\n",
    "del values\n",
    "gc.collect()\n",
    "submission4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission123_needed = submission123.drop(label_cols, axis=1)\n",
    "submission4_needed = submission4.drop(org4_cols, axis=1)\n",
    "submission1234 = pd.concat([submission123_needed, submission4_needed])\n",
    "print(submission1234.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission123_combined_needed = submission123_combined.drop(label_cols, axis=1)\n",
    "submission4_needed = submission4.drop(org4_cols, axis=1)\n",
    "submission1234_combined = pd.concat([submission123_combined_needed, submission4_needed])\n",
    "print(submission1234_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1234_combined = submission1234_combined.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1234.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1234.to_csv('data/sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1234_combined.to_csv('data/sample_submission_combined.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train123 = pd.read_csv(\"data/train123.csv\")\n",
    "test123 = pd.read_csv(\"data/test123.csv\")\n",
    "train123.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train123['label_one'] = train123['labels'].apply(lambda x: x[0])\n",
    "train123.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train123['label_one'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "               min_df=5, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, max_features=50000,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_tfidf = word_vectorizer.fit_transform(train123['text'])\n",
    "test_tfidf = word_vectorizer.transform(test123['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train123['label_one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "import pickle\n",
    "\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "lr = GridSearchCV(LogisticRegression(random_state=23), hyperparameters, cv=5, n_jobs=-1, verbose=0)\n",
    "print(\"[INFO]Training..\")\n",
    "lr.fit(train_tfidf, y_train)\n",
    "\n",
    "print(\"[INFO]Saving...\")\n",
    "with open('logisticRegression_model.pickle', 'wb') as file:\n",
    "    pickle.dump(lr, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(test_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test123['model_lr'] = lr.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test123[\"id\"]})\n",
    "submission['predicted_label'] = test123['model_lr']\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission4_needed = submission4.drop(org4_cols, axis=1)\n",
    "submission_all = pd.concat([submission, submission4_needed])\n",
    "print(submission_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_all.to_csv('data/simple_LR.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission4_needed.to_csv('data/submission4.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
